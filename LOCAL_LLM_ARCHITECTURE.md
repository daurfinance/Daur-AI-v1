# ü§ñ Local LLM Architecture for MVP

## Goal
**100% –ª–æ–∫–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –±–µ–∑ API** - –≤—Å–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–µ –∫–ª–∏–µ–Ω—Ç–∞

---

## üÜì –õ–æ–∫–∞–ª—å–Ω–∞—è –Ø–∑—ã–∫–æ–≤–∞—è –ú–æ–¥–µ–ª—å

### –í—ã–±–æ—Ä: Ollama + Llama 3.2

**–ü–æ—á–µ–º—É Ollama?**
- ‚úÖ –ë–µ—Å–ø–ª–∞—Ç–Ω–æ –∏ open source
- ‚úÖ –ü—Ä–æ—Å—Ç–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ (–æ–¥–Ω–∞ –∫–æ–º–∞–Ω–¥–∞)
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ macOS, Windows, Linux
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏
- ‚úÖ REST API –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ vision –º–æ–¥–µ–ª–µ–π

**–ü–æ—á–µ–º—É Llama 3.2?**
- ‚úÖ –õ—É—á—à–∞—è open source –º–æ–¥–µ–ª—å (–æ—Ç Meta)
- ‚úÖ –•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ reasoning
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ vision (Llama 3.2-Vision)
- ‚úÖ –†–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã (1B, 3B, 11B, 90B)
- ‚úÖ –ë—ã—Å—Ç—Ä–∞—è —Ä–∞–±–æ—Ç–∞

---

## üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞

### –®–∞–≥ 1: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Ollama

**macOS:**
```bash
brew install ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Windows:**
```powershell
# –°–∫–∞—á–∞—Ç—å —Å https://ollama.com/download
```

### –®–∞–≥ 2: –°–∫–∞—á–∞—Ç—å –ú–æ–¥–µ–ª–∏

**–û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å (—Ç–µ–∫—Å—Ç):**
```bash
# Llama 3.2 3B (–±—ã—Å—Ç—Ä–∞—è, –¥–ª—è –æ–±—ã—á–Ω—ã—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–≤)
ollama pull llama3.2:3b

# –ò–ª–∏ Llama 3.2 11B (–ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω—É–∂–µ–Ω –º–æ—â–Ω—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä)
ollama pull llama3.2:11b
```

**Vision –º–æ–¥–µ–ª—å (–∞–Ω–∞–ª–∏–∑ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤):**
```bash
# Llama 3.2-Vision 11B
ollama pull llama3.2-vision:11b

# –ò–ª–∏ LLaVA (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞)
ollama pull llava
```

**Coding –º–æ–¥–µ–ª—å (–¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞):**
```bash
# CodeLlama (—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ –∫–æ–¥–µ)
ollama pull codellama:7b
```

---

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

### –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã

```
Local AI Stack:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Daur AI MVP Agent           ‚îÇ
‚îÇ  (Python, –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç –≤—Å–µ)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          Ollama Server              ‚îÇ
‚îÇ  (–£–ø—Ä–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º–∏)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Local Models                ‚îÇ
‚îÇ  ‚Ä¢ Llama 3.2 (reasoning)            ‚îÇ
‚îÇ  ‚Ä¢ Llama 3.2-Vision (screen)        ‚îÇ
‚îÇ  ‚Ä¢ CodeLlama (coding)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Client Hardware (GPU/CPU)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üíª –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –ö–æ–¥–µ

### –ë–∞–∑–æ–≤–æ–µ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

```python
import requests
import json

class LocalLLM:
    def __init__(self, model="llama3.2:3b"):
        self.model = model
        self.api_url = "http://localhost:11434/api/generate"
    
    def chat(self, prompt: str) -> str:
        """–û—Ç–ø—Ä–∞–≤–∏—Ç—å –ø—Ä–æ–º–ø—Ç –≤ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å"""
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False
        }
        
        response = requests.post(self.api_url, json=payload)
        result = response.json()
        return result['response']
    
    def chat_with_vision(self, prompt: str, image_path: str) -> str:
        """–ê–Ω–∞–ª–∏–∑ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞ –ª–æ–∫–∞–ª—å–Ω–æ"""
        # –ß–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        import base64
        with open(image_path, 'rb') as f:
            image_data = base64.b64encode(f.read()).decode()
        
        payload = {
            "model": "llama3.2-vision:11b",
            "prompt": prompt,
            "images": [image_data],
            "stream": False
        }
        
        response = requests.post(self.api_url, json=payload)
        result = response.json()
        return result['response']

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
llm = LocalLLM()

# –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
response = llm.chat("What should I do to open Safari?")
print(response)

# –ê–Ω–∞–ª–∏–∑ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞
analysis = llm.chat_with_vision(
    prompt="What applications are visible on this screen?",
    image_path="/path/to/screenshot.png"
)
print(analysis)
```

---

## üéØ –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ú–æ–¥–µ–ª–∏

### 1. Reasoning Model (Llama 3.2)

**–î–ª—è:** –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π

```python
reasoning_llm = LocalLLM(model="llama3.2:3b")

plan = reasoning_llm.chat("""
Task: Open Safari and search for "AI automation"

Create a step-by-step plan:
""")

print(plan)
# Output:
# 1. Press Cmd+Space to open Spotlight
# 2. Type "Safari"
# 3. Press Enter
# 4. Wait for Safari to open
# 5. Press Cmd+L to focus address bar
# 6. Type "AI automation"
# 7. Press Enter
```

---

### 2. Vision Model (Llama 3.2-Vision)

**–î–ª—è:** –ê–Ω–∞–ª–∏–∑ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤, –ø–æ–Ω–∏–º–∞–Ω–∏–µ UI

```python
vision_llm = LocalLLM(model="llama3.2-vision:11b")

# –ê–Ω–∞–ª–∏–∑ —ç–∫—Ä–∞–Ω–∞
screen_analysis = vision_llm.chat_with_vision(
    prompt="What is the current state of the screen? List all visible UI elements.",
    image_path="screenshot.png"
)

# –ü–æ–∏—Å–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
button_location = vision_llm.chat_with_vision(
    prompt="Where is the 'New Tab' button located? Provide coordinates.",
    image_path="screenshot.png"
)
```

---

### 3. Coding Model (CodeLlama)

**–î–ª—è:** –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞, —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥

```python
code_llm = LocalLLM(model="codellama:7b")

code = code_llm.chat("""
Write a Python function that:
1. Takes a screenshot
2. Saves it to a file
3. Returns the file path

Use pyautogui library.
""")

print(code)
# Output: –ø–æ–ª–Ω—ã–π —Ä–∞–±–æ—á–∏–π –∫–æ–¥ —Ñ—É–Ω–∫—Ü–∏–∏
```

---

## ‚ö° –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

### –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ñ–µ–ª–µ–∑—É

| –ú–æ–¥–µ–ª—å | RAM | GPU | –°–∫–æ—Ä–æ—Å—Ç—å (tokens/sec) |
|--------|-----|-----|----------------------|
| Llama 3.2 1B | 2GB | –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ | 50-100 |
| Llama 3.2 3B | 4GB | –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ | 30-60 |
| Llama 3.2 11B | 8GB | –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è | 10-30 |
| Llama 3.2-Vision 11B | 12GB | –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è | 5-15 |
| CodeLlama 7B | 6GB | –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ | 20-40 |

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

**–î–ª—è —Å–ª–∞–±—ã—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–≤:**
```bash
# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏, –±—ã—Å—Ç—Ä–µ–µ)
ollama pull llama3.2:3b-q4_0  # 4-bit quantization
```

**–î–ª—è –º–æ—â–Ω—ã—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–≤:**
```bash
# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–æ–ª–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ)
ollama pull llama3.2:11b
```

---

## üîÑ Fallback –°—Ç—Ä–∞—Ç–µ–≥–∏—è

### –ì–∏–±—Ä–∏–¥–Ω—ã–π –ü–æ–¥—Ö–æ–¥

```python
class HybridLLM:
    def __init__(self):
        self.local_llm = LocalLLM(model="llama3.2:3b")
        self.use_cloud = False  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ª–æ–∫–∞–ª—å–Ω–æ
    
    async def chat(self, prompt: str) -> str:
        try:
            # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ
            response = self.local_llm.chat(prompt)
            return response
        except Exception as e:
            # –ï—Å–ª–∏ –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
            if self.use_cloud:
                # Fallback –∫ cloud API (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                response = await openai.chat(prompt)
                return response
            else:
                raise e
```

---

## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: Local vs Cloud

| –ü–∞—Ä–∞–º–µ—Ç—Ä | Local (Ollama) | Cloud (OpenAI) |
|----------|----------------|----------------|
| **–°—Ç–æ–∏–º–æ—Å—Ç—å** | $0 (–±–µ—Å–ø–ª–∞—Ç–Ω–æ) | $0.01-0.05 –∑–∞ –∑–∞–ø—Ä–æ—Å |
| **–°–∫–æ—Ä–æ—Å—Ç—å** | 1-5 —Å–µ–∫ | 2-6 —Å–µ–∫ |
| **–ö–∞—á–µ—Å—Ç–≤–æ** | –•–æ—Ä–æ—à–µ–µ (80-90%) | –û—Ç–ª–∏—á–Ω–æ–µ (95-99%) |
| **–ü—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å** | 100% –ø—Ä–∏–≤–∞—Ç–Ω–æ | –î–∞–Ω–Ω—ã–µ —É—Ö–æ–¥—è—Ç –≤ cloud |
| **–û—Ñ—Ñ–ª–∞–π–Ω** | ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç | ‚ùå –ù—É–∂–µ–Ω –∏–Ω—Ç–µ—Ä–Ω–µ—Ç |
| **–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è** | 4-12GB RAM | –¢–æ–ª—å–∫–æ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç |

---

## üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –¥–ª—è MVP

### –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

**–î–ª—è –æ–±—ã—á–Ω—ã—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–≤ (8GB RAM):**
```bash
ollama pull llama3.2:3b        # –û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å
ollama pull llava              # Vision (–ª–µ–≥—á–µ —á–µ–º Llama 3.2-Vision)
```

**–î–ª—è –º–æ—â–Ω—ã—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–≤ (16GB+ RAM):**
```bash
ollama pull llama3.2:11b           # –û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å
ollama pull llama3.2-vision:11b    # Vision
ollama pull codellama:7b           # Coding
```

---

## üöÄ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –õ–æ–∫–∞–ª—å–Ω–æ–≥–æ –†–µ—à–µ–Ω–∏—è

1. **–ë–µ—Å–ø–ª–∞—Ç–Ω–æ** - –Ω–µ—Ç API costs
2. **–ü—Ä–∏–≤–∞—Ç–Ω–æ** - –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –æ—Å—Ç–∞—é—Ç—Å—è –Ω–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–µ
3. **–ë—ã—Å—Ç—Ä–æ** - –Ω–µ—Ç —Å–µ—Ç–µ–≤—ã—Ö –∑–∞–¥–µ—Ä–∂–µ–∫
4. **–û—Ñ—Ñ–ª–∞–π–Ω** - —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
5. **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ** - –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å —Å–∫–æ–ª—å–∫–æ —É–≥–æ–¥–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤
6. **–ö–∞—Å—Ç–æ–º–∏–∑–∏—Ä—É–µ–º–æ** - –º–æ–∂–Ω–æ fine-tune –º–æ–¥–µ–ª–∏

---

## üìù –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–ª—è –ö–ª–∏–µ–Ω—Ç–∞

### –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –°–∫—Ä–∏–ø—Ç

```bash
#!/bin/bash
# setup_local_ai.sh

echo "ü§ñ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ AI –¥–ª—è Daur MVP..."

# 1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Ollama
echo "üì¶ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é Ollama..."
brew install ollama

# 2. –ó–∞–ø—É—Å—Ç–∏—Ç—å Ollama —Å–µ—Ä–≤–µ—Ä
echo "üöÄ –ó–∞–ø—É—Å–∫–∞—é Ollama —Å–µ—Ä–≤–µ—Ä..."
ollama serve &

# 3. –°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª–∏
echo "‚¨áÔ∏è –°–∫–∞—á–∏–≤–∞—é –º–æ–¥–µ–ª–∏ (—ç—Ç–æ –∑–∞–π–º–µ—Ç 5-10 –º–∏–Ω—É—Ç)..."
ollama pull llama3.2:3b
ollama pull llava

echo "‚úÖ –ì–æ—Ç–æ–≤–æ! –õ–æ–∫–∞–ª—å–Ω—ã–π AI —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!"
```

---

## üéâ –ò—Ç–æ–≥

**MVP –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
- ‚úÖ **Ollama** - —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏
- ‚úÖ **Llama 3.2** - reasoning –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
- ‚úÖ **LLaVA** - –∞–Ω–∞–ª–∏–∑ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤
- ‚úÖ **CodeLlama** (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞

**–í—Å–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ, –±–µ—Å–ø–ª–∞—Ç–Ω–æ, –ø—Ä–∏–≤–∞—Ç–Ω–æ!** üöÄ

